

import asyncio
import requests

from lib.utils.parsers.html import parse_urls

class Crawler(object):
    """
    A simple crawler that accepts in input a start_url, max_depth and crawls the website asynchronously, yielding HTML pages.
    """

    
 
    def __init__(self, start_url, max_depth = 1):
        self.start_url = start_url
        self.max_depth = max_depth
        self.depth = 0
        self.visited = set()
        self.pages = []

    async def _crawl(self, url):
        if self.depth <= self.max_depth:
            self.depth += 1
            self.visited.add(url)
            print(f"Crawling {url} at depth {self.depth}")
            page = await asyncio.get_event_loop().run_in_executor(None, requests.get, url)
            self.pages.append(page.text)
            for link in self.get_links(page.text):
                if link not in self.visited:
                    await self.crawl(link)
            self.depth -= 1

    def get_links(self, html):
        return parse_urls(html)

    @classmethod
    def crawl(cls, url):
        loop =asyncio.get_event_loop()
        c = cls()
        loop.run_until_complete(
            c.crawl(url)
        )
        return c.pages

    # Nodes is dictionary of dictionaries whereas each URL contains a dictionary of its outgoing links.
    nodes = {}

